{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "Analysis of bacterial data (wild type and mutants cells) with EfficientNet features and cell areas.\n",
    "\n",
    "**Pipeline Overview:**\n",
    "- **Step 1**: Pre-processing (merge data, add labels, filter low area, remove constant columns, outlier detection).\n",
    "- **Step 2**: Subset selection.\n",
    "- **Step 3**: Data scaling (standardize WT, save scaler).\n",
    "- **Step 4**: PHATE\n",
    "- **Step 5**: Angle aperture plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/leuven/359/vsc35907/miniconda3/envs/omnipose/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import phate\n",
    "import scanpy as sc\n",
    "import scvelo as scv\n",
    "from scipy.optimize import least_squares\n",
    "from matplotlib.patches import Circle, Arc\n",
    "from matplotlib.patches import Ellipse, Arc\n",
    "import hdbscan\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('/scratch/leuven/359/vsc35907/big_data_feature_extraction/analysis/results/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "feature_file = '/scratch/leuven/359/vsc35907/big_data_feature_extraction/patches_dirs/features_model_lr_3e3_adamw_wd_1e5.csv'\n",
    "area_file = '/scratch/leuven/359/vsc35907/big_data_feature_extraction/patches_dirs/area_cells.csv'\n",
    "output_dir = '/scratch/leuven/359/vsc35907/big_data_feature_extraction/patches_dirs/clean_data'\n",
    "garbage_dir = '/scratch/leuven/359/vsc35907/big_data_feature_extraction/analysis/garbage_cells'\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(garbage_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 20000\n",
    "\n",
    "# Helper functions\n",
    "def normalize_path(path):\n",
    "    \"\"\"Normalize file paths to ensure consistency.\"\"\"\n",
    "    dirs = path.split(\"/\")\n",
    "    if len(dirs) == 3:\n",
    "        path = '/scratch/leuven/359/vsc35907/big_data_feature_extraction/patches_dirs/' + path\n",
    "    return path\n",
    "\n",
    "def extract_batch(path):\n",
    "    return int(os.path.basename(path).split()[0][:-3])\n",
    "\n",
    "def extract_group(path):\n",
    "    basename = os.path.basename(path)\n",
    "    directory = os.path.dirname(path)\n",
    "    pieces = basename.split()\n",
    "    if len(pieces) == 5:\n",
    "        group = pieces[3]\n",
    "    else:\n",
    "        group = pieces[2]\n",
    "    return int(group)\n",
    "\n",
    "def process_and_save(df, key, gene_mapping, output_dir):   \n",
    "    \n",
    "    # Add gene info\n",
    "    df['gene'] = np.where(\n",
    "        df['label'] == 1,\n",
    "        df['group'].map(gene_mapping),\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Save to file\n",
    "    if not df.empty:\n",
    "        output_file = os.path.join(output_dir, f\"clean_{key[0]}_{key[1]}_{key[2]}.csv\")\n",
    "        df.to_csv(\n",
    "            output_file,\n",
    "            mode='a',  # append if file exists\n",
    "            header=not os.path.exists(output_file),  # write header only if file does not exist\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"Saved combination {key} to {output_file}, Shape: {df.shape}\")\n",
    "    else:\n",
    "        print(f\"Combination {key} has no rows after preprocessing, skipping save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load gene mapping\n",
    "gene_df = pd.read_csv('/data/leuven/359/vsc35907/EfficientNet_feature_extraction/mutant_names.csv')\n",
    "gene_mapping = gene_df.set_index('group')['gene'].to_dict()\n",
    "\n",
    "# Build area dictionary with normalized paths\n",
    "area_dict = {}\n",
    "for area_chunk in pd.read_csv(area_file, chunksize=chunk_size):\n",
    "    for _, row in area_chunk.iterrows():\n",
    "        area_dict[normalize_path(row['image_path'])] = row['area']\n",
    "print(f\"Built area_dict with {len(area_dict)} entries\")\n",
    "\n",
    "# Initialize accumulators\n",
    "combo_dfs = {}\n",
    "prev_combos = set()\n",
    "\n",
    "# Process feature file in chunks\n",
    "for i, feature_chunk in enumerate(pd.read_csv(feature_file, chunksize=chunk_size)):\n",
    "    \n",
    "    print(f\"\\nProcessing chunk {i+1}...\")\n",
    "    \n",
    "    # if i < 20:\n",
    "    #     continue\n",
    "    \n",
    "    # Normalize image_path in feature_chunk\n",
    "    feature_chunk['image_path'] = feature_chunk['image_path'].apply(normalize_path)\n",
    "\n",
    "    # Merge with area data\n",
    "    feature_chunk['area'] = feature_chunk['image_path'].map(area_dict)\n",
    "\n",
    "    # Debug: Check for unmatched image_path\n",
    "    unmatched = feature_chunk[feature_chunk['area'].isna()]['image_path']\n",
    "    if not unmatched.empty:\n",
    "        print(f\"Warning: {len(unmatched)} unmatched image_path in chunk {i+1}. Sample: {unmatched.iloc[:5].tolist()}\")\n",
    "\n",
    "    \n",
    "    print(f\"Removing rows with any NaN, shape: {feature_chunk.shape}\")\n",
    "    feature_chunk = feature_chunk.dropna()\n",
    "    print(f\"After removing rows with any NaN, shape: {feature_chunk.shape}\")\n",
    "\n",
    "    # Warn if all rows were removed\n",
    "    if feature_chunk.empty:\n",
    "        print(f\"Warning: All rows removed in chunk {i+1}. Check data consistency.\")\n",
    "        continue\n",
    "\n",
    "    # Add metadata\n",
    "    feature_chunk['label'] = feature_chunk['image_path'].apply(lambda x: 1 if 'mutant' in x.lower() else 0)\n",
    "    feature_chunk['batch'] = feature_chunk['image_path'].apply(extract_batch)\n",
    "    feature_chunk['group'] = feature_chunk['image_path'].apply(extract_group)\n",
    "\n",
    "    # Move image_path to end\n",
    "    image_path = feature_chunk.pop('image_path')\n",
    "    feature_chunk['image_path'] = image_path\n",
    "\n",
    "    # Remove garbage cells\n",
    "    garbage_df = feature_chunk[feature_chunk['area'] <= 270]\n",
    "    for img in garbage_df['image_path']:\n",
    "        shutil.copy2(img, os.path.join(garbage_dir, os.path.basename(img)))\n",
    "    feature_chunk = feature_chunk[feature_chunk['area'] > 270]\n",
    "    print(f\"Removed {len(garbage_df)} low-area cells in chunk {i+1}\")\n",
    "    print(f\"Shape: {feature_chunk.shape}\")\n",
    "\n",
    "    # Group by combination\n",
    "    grouped = feature_chunk.groupby(['label', 'batch', 'group'])\n",
    "    current_combos = set(grouped.groups.keys())\n",
    "\n",
    "    # Accumulate data\n",
    "    for key, group_df in grouped:\n",
    "        if key in combo_dfs:\n",
    "            combo_dfs[key] = pd.concat([combo_dfs[key], group_df])\n",
    "        else:\n",
    "            combo_dfs[key] = group_df.copy()\n",
    "\n",
    "    # Process completed combinations\n",
    "    completed_combos = prev_combos - current_combos\n",
    "    for key in completed_combos:\n",
    "        process_and_save(combo_dfs[key], key, gene_mapping, output_dir)\n",
    "        del combo_dfs[key]\n",
    "    prev_combos = current_combos\n",
    "    print(f\"Chunk {i+1} processed, active combinations: {len(combo_dfs)}\")\n",
    "\n",
    "# Process remaining combinations\n",
    "for key in list(combo_dfs.keys()):\n",
    "    process_and_save(combo_dfs[key], key, gene_mapping, output_dir)\n",
    "    del combo_dfs[key]\n",
    "\n",
    "print(\"Preprocessing complete. Cleaned files saved in\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETES ALL THE CSV FILES IN THE OUTPUT DIRECTORY!!!!!!!!!!\n",
    "# DO NOT RUN THIS CELL!!! I REPET: DO NOT RUN THIS CELL!!!!!!\n",
    "\n",
    "# import os\n",
    "\n",
    "# for filename in os.listdir(output_dir):\n",
    "#     file_path = os.path.join(output_dir, filename)\n",
    "#     if os.path.isfile(file_path):\n",
    "#         os.remove(file_path)\n",
    "# print(os.listdir(output_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check csv files shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(output_dir)\n",
    "files = [f for f in files if '.csv' in f]\n",
    "print(f'total csv files found: {len(files)}')\n",
    "for filename in files:\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f'file: {os.path.basename(file_path)} shape: {df.shape}')  # Remove the parentheses\n",
    "#print(os.listdir(output_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 2-7: Subset Selection, Normalization, Outliers Filtering, PHATE, and Aperture Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.optimize import least_squares\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Arc as MplArc\n",
    "\n",
    "def fit_parabola(points):\n",
    "    \"\"\"Fit a parabolic arc (y = ax^2 + bx + c) to 2D points using least-squares.\"\"\"\n",
    "    def parabola_residuals(params, x, y):\n",
    "        a, b, c = params\n",
    "        y_pred = a * x**2 + b * x + c\n",
    "        return y - y_pred\n",
    "\n",
    "    x, y = points[:, 0], points[:, 1]\n",
    "    xc, yc = np.mean(x), np.mean(y)\n",
    "    a_guess = 0.1\n",
    "    b_guess = 0.0\n",
    "    c_guess = yc - a_guess * xc**2\n",
    "    initial_guess = [a_guess, b_guess, c_guess]\n",
    "\n",
    "    result = least_squares(parabola_residuals, initial_guess, args=(x, y))\n",
    "    a, b, c = result.x\n",
    "    return xc, yc, a, b, c\n",
    "\n",
    "def compute_arc_length(points):\n",
    "    \"\"\"Compute the arc length of the 2D curve defined by the given points.\"\"\"\n",
    "    diffs = np.diff(points, axis=0)\n",
    "    distances = np.linalg.norm(diffs, axis=1)\n",
    "    return np.sum(distances)\n",
    "\n",
    "def compute_width(points):\n",
    "    \"\"\"Compute the width (x-span) of the points.\"\"\"\n",
    "    return np.max(points[:, 0]) - np.min(points[:, 0])\n",
    "\n",
    "def compute_height(points):\n",
    "    \"\"\"Compute the height (y-span) of the points.\"\"\"\n",
    "    return np.max(points[:, 1]) - np.min(points[:, 1])\n",
    "\n",
    "def compute_center(points):\n",
    "    \"\"\"Compute the geometric center of the points.\"\"\"\n",
    "    return np.mean(points[:, 0]), np.mean(points[:, 1])\n",
    "\n",
    "def visualize_horseshoe(points, areas, cell_stages, xc, yc, a, b, c, curvature, batch, group, label_str, save_dir):\n",
    "    \"\"\"Visualize PHATE points colored by area and cell stage, with fitted parabolic arc.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    sc_area = ax.scatter(points[:, 0], points[:, 1], c=areas, s=50, alpha=0.3, cmap='viridis', label='Area')\n",
    "    plt.colorbar(sc_area, label='Area', shrink=0.75)\n",
    "\n",
    "    cell_stages = np.array(cell_stages)\n",
    "    non_nan_mask = ~pd.isna(cell_stages)\n",
    "    if non_nan_mask.sum() > 0:\n",
    "        stage_dict = {\n",
    "            'B': '#03ecfc',\n",
    "            'C': '#7bfc03',\n",
    "            'D': '#fc03d3'\n",
    "        }\n",
    "        unique_stages = np.unique(cell_stages[non_nan_mask])\n",
    "        for stage in unique_stages:\n",
    "            if stage in stage_dict:\n",
    "                stage_mask = (cell_stages == stage) & non_nan_mask\n",
    "                ax.scatter(\n",
    "                    points[stage_mask, 0], points[stage_mask, 1],\n",
    "                    c=[stage_dict[stage]], s=50, label=f'Cell Stage {stage}', alpha=0.8\n",
    "                )\n",
    "\n",
    "    x_range = np.linspace(np.min(points[:, 0]), np.max(points[:, 0]), 100)\n",
    "    y_parabola = a * x_range**2 + b * x_range + c\n",
    "    ax.plot(x_range, y_parabola, color='red', linestyle='--', linewidth=2, label='Fitted parabola')\n",
    "\n",
    "    ax.scatter([xc], [yc], c='green', s=100, marker='x', label='Centroid')\n",
    "\n",
    "    data_min = np.min(points)\n",
    "    data_max = np.max(points)\n",
    "    padding = (data_max - data_min) * 0.1\n",
    "    axis_limit_min = data_min - padding\n",
    "    axis_limit_max = data_max + padding\n",
    "    ax.set_xlim(axis_limit_min, axis_limit_max)\n",
    "    ax.set_ylim(axis_limit_min, axis_limit_max)\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel('PHATE 1')\n",
    "    ax.set_ylabel('PHATE 2')\n",
    "    ax.legend(bbox_to_anchor=(0.5, -0.15), loc='upper center', ncol=4, frameon=False)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.title(f'PHATE Plot: Label {label_str}, Batch {batch}, Group {group}')\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plot_path = os.path.join(save_dir, f'horseshoe_label_{label_str}_batch_{batch}_group_{group}.png')\n",
    "    plt.savefig(plot_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def analyze_horseshoe(points, areas, cell_stages, batch, group, label_str, save_dir):\n",
    "    \"\"\"Analyze a horseshoe-shaped data cloud and save visualization.\"\"\"\n",
    "    if not isinstance(points, np.ndarray) or points.shape[1] != 2:\n",
    "        raise ValueError(\"Input must be a NumPy array with shape (n, 2)\")\n",
    "\n",
    "    xc, yc, a, b, c = fit_parabola(points)\n",
    "    curvature = a  # This is the \"openness\" of the parabola\n",
    "\n",
    "    arc_length = compute_arc_length(points)\n",
    "    width = compute_width(points)\n",
    "    height = compute_height(points)\n",
    "    center_x, center_y = compute_center(points)\n",
    "\n",
    "    visualize_horseshoe(points, areas, cell_stages, xc, yc, a, b, c, curvature, batch, group, label_str, save_dir)\n",
    "\n",
    "    return {\n",
    "        'centroid': (xc, yc),\n",
    "        'parabola_coefficients': {'a': a, 'b': b, 'c': c},\n",
    "        'curvature': curvature,\n",
    "        'arc_length': arc_length,\n",
    "        'width': width,\n",
    "        'height': height,\n",
    "        'geometric_center': (center_x, center_y)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subset(df, feature_columns, label_str, batch, group, results_dir, outliers_excluded=True):\n",
    "    \"\"\"Process a subset of the DataFrame for a given label, batch, and group.\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nProcessing Label {label_str}, Batch {batch}, Group {group}...\")\n",
    "        if df.shape[0] < 100:\n",
    "            print(f'Skipping Label {label_str}, Batch {batch}, Group {group}: Insufficient data (n={df.shape[0]})')\n",
    "            return None\n",
    "        \n",
    "        print(f'Initial shape: {df.shape}')\n",
    "        \n",
    "        # Step 1: Initial Outlier Removal\n",
    "        features = df[feature_columns].values\n",
    "        inlier_mask_initial = np.all(\n",
    "            np.abs((features - np.mean(features, axis=0)) / np.std(features, axis=0)) <= 12,\n",
    "            axis=1\n",
    "        )\n",
    "        inlier_indices = df.index[inlier_mask_initial]\n",
    "        features = features[inlier_mask_initial]\n",
    "        print(f'Shape after outlier removal: {features.shape}')\n",
    "\n",
    "        # Step 2: Normalization\n",
    "        scaler = StandardScaler()\n",
    "        features = scaler.fit_transform(features)\n",
    "        \n",
    "        # Step 3: PHATE\n",
    "        phate_op = phate.PHATE(n_components=2, random_state=42, n_jobs=-1, verbose=False)\n",
    "        phate_data = phate_op.fit_transform(features)\n",
    "        print(f'PHATE shape: {phate_data.shape}')\n",
    "        unique_points = np.unique(phate_data, axis=0).shape[0]\n",
    "        print(f'Number of unique PHATE points: {unique_points}')\n",
    "\n",
    "        # Step 4: Select additional data using final indices\n",
    "        areas = df.loc[inlier_indices, 'area'].values\n",
    "        cell_stages = df.loc[inlier_indices, 'cell_stage'].values\n",
    "        print(f'Areas shape: {areas.shape}')\n",
    "        print(f'Cell stages shape: {cell_stages.shape}')\n",
    "\n",
    "        # Step 5: Shape analysis (curvature + visualization)\n",
    "        result = analyze_horseshoe(phate_data, areas, cell_stages, batch, group, label_str, results_dir)\n",
    "        print(result)\n",
    "\n",
    "        result.update({'batch': batch, 'group': group, 'label': label_str})\n",
    "        print(f'Centroid: {result[\"centroid\"]}')\n",
    "        print(f'Curvature (a): {result[\"curvature\"]:.4f}')\n",
    "        print(f'Arc length: {result[\"arc_length\"]:.2f}')\n",
    "        print(f'Width: {result[\"width\"]:.2f}')\n",
    "        print(f'Height: {result[\"height\"]:.2f}')\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error processing Label {label_str}, Batch {batch}, Group {group}: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run analysis on each individual group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Results directory\n",
    "results_dir = '/scratch/leuven/359/vsc35907/big_data_feature_extraction/results/phate_plots2'\n",
    "csv_dir = '/scratch/leuven/359/vsc35907/big_data_feature_extraction/patches_dirs/clean_data'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Checkpoint file\n",
    "checkpoint_file = os.path.join(results_dir, 'checkpoint.txt')\n",
    "\n",
    "# Load processed files from checkpoint\n",
    "processed_files = set()\n",
    "try:\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            processed_files = set(line.strip() for line in f if line.strip())\n",
    "        print(f\"Loaded {len(processed_files)} processed files from checkpoint\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading checkpoint file: {e}\")\n",
    "\n",
    "feature_columns = [f'feature_{i}' for i in range(1280)]\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Process each label\n",
    "for file in os.listdir(csv_dir):  # Limited to first two files; remove [:2] for all files\n",
    "    if file in processed_files:\n",
    "        print(f\"Skipping already processed file: {file}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        _, label_str, batch, group = file.split(\"_\")\n",
    "        group = group[:-4]\n",
    "    except ValueError:\n",
    "        print(f\"Skipping file with invalid name format: {file}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(csv_dir, file))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Add cell_stage column initialized to NaN\n",
    "    df['cell_stage'] = np.nan\n",
    "    \n",
    "    # Process the subset\n",
    "    result = process_subset(df, feature_columns, label_str, batch, group, results_dir)\n",
    "    if result is not None:\n",
    "        results.append(result)\n",
    "        # Update checkpoint\n",
    "        try:\n",
    "            with open(checkpoint_file, 'a') as f:\n",
    "                f.write(f\"{file}\\n\")\n",
    "            processed_files.add(file)\n",
    "            print(f\"Added {file} to checkpoint\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating checkpoint for {file}: {e}\")\n",
    "\n",
    "# Save results to CSV\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    try:\n",
    "        results_df.to_csv(os.path.join(results_dir, 'aperture_results.csv'), index=False)\n",
    "        print('\\nAnalysis complete. Results saved to aperture_results.csv')\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results CSV: {e}\")\n",
    "else:\n",
    "    print('\\nNo new results to save.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download The Images\n",
    "\n",
    "```\n",
    "scp 'vsc35907@login.hpc.kuleuven.be:/scratch/leuven/359/vsc35907/big_data_feature_extraction/results/phate_plots2/*' /Users/theo/Desktop/phate_plots2.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Cell Cycle Trajectories Fitted Parabolas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    df = pd.read_csv(os.path.join(results_dir, 'aperture_results.csv'), sep=',')\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Parse columns\n",
    "try:\n",
    "    df['parabola_coefficients'] = df['parabola_coefficients'].apply(ast.literal_eval)\n",
    "    df['centroid'] = df['centroid'].apply(ast.literal_eval)\n",
    "except Exception as e:\n",
    "    print(f\"Error parsing columns: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# Color by label (assuming 0=Wild Type, 1=Mutant; adjust if labels are strings)\n",
    "colors = {'0': 'green', '1': 'blue'}  # Update keys based on actual label values\n",
    "\n",
    "# Track min/max for axis limits\n",
    "x_all, y_all = [], []\n",
    "\n",
    "# Plot each parabola and centroid\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        label = row['label']\n",
    "        color = colors.get(str(label), 'gray')  # Convert to string, default to gray if not in colors\n",
    "        \n",
    "        # Extract parabola coefficients\n",
    "        coeffs = row['parabola_coefficients']\n",
    "        a, b, c = coeffs['a'], coeffs['b'], coeffs['c']\n",
    "        \n",
    "        # Use width for x-range, centered at centroid\n",
    "        x_centroid, y_centroid = row['centroid']\n",
    "        width = row['width']\n",
    "        x_vals = np.linspace(-width/2, width/2, 300) + x_centroid\n",
    "        y_vals = a * x_vals**2 + b * x_vals + c\n",
    "        \n",
    "        # Plot parabola\n",
    "        ax.plot(x_vals, y_vals, color=color, linewidth=2, alpha=0.7)\n",
    "        ax.scatter(x_centroid, y_centroid, color=color, marker='x', s=50)\n",
    "        \n",
    "        # Annotate with group (kept as in original)\n",
    "        ax.text(x_centroid, y_centroid, str(row['group']), color=color, ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        # Collect coordinates for axis limits\n",
    "        x_all.extend(x_vals)\n",
    "        y_all.extend(y_vals)\n",
    "        x_all.append(x_centroid)\n",
    "        y_all.append(y_centroid)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Axes and layout\n",
    "ax.set_xlabel('PHATE 1')\n",
    "ax.set_ylabel('PHATE 2')\n",
    "ax.set_title('PHATE Parabolic Fits with Centroids')\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Set axis limits with padding\n",
    "padding = 0.1\n",
    "x_min, x_max = min(x_all), max(x_all)\n",
    "y_min, y_max = min(y_all), max(y_all)\n",
    "x_pad = (x_max - x_min) * padding\n",
    "y_pad = (y_max - y_min) * padding\n",
    "ax.set_xlim(x_min - x_pad, x_max + x_pad)\n",
    "ax.set_ylim(y_min - y_pad, y_max + y_pad)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='green', lw=2, label='Wild Type'),\n",
    "    Line2D([0], [0], color='blue', lw=2, label='Mutant')\n",
    "]\n",
    "ax.legend(handles=legend_elements)\n",
    "\n",
    "# Save and show plot\n",
    "try:\n",
    "    plt.savefig(os.path.join(results_dir, 'phate_parabolas.png'), \n",
    "                bbox_inches='tight', dpi=300)\n",
    "except Exception as e:\n",
    "    print(f\"Error saving plot: {e}\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair Plot Fitted Elipses data variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv(os.path.join(results_dir, 'aperture_results.csv'), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Drop only 'batch' and 'group', keep 'label' for coloring\n",
    "df = df.drop(columns=['batch', 'group'])\n",
    "\n",
    "# Create a scatter matrix with points colored by 'label'\n",
    "sns.pairplot(df, hue='label')\n",
    "plt.suptitle('Scatter Matrix of Variables Colored by Label', y=1.02)\n",
    "plt.savefig(os.path.join(results_dir, 'pair_plot.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a composite image of the many phate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /scratch/leuven/359/vsc35907/big_data_feature_extraction/results/phate_plots2/composite/grid_label_1_1.png\n",
      "Saved /scratch/leuven/359/vsc35907/big_data_feature_extraction/results/phate_plots2/composite/grid_label_1_2.png\n",
      "Saved /scratch/leuven/359/vsc35907/big_data_feature_extraction/results/phate_plots2/composite/grid_label_1_3.png\n",
      "Saved /scratch/leuven/359/vsc35907/big_data_feature_extraction/results/phate_plots2/composite/grid_label_1_4.png\n",
      "Saved /scratch/leuven/359/vsc35907/big_data_feature_extraction/results/phate_plots2/composite/grid_label_1_5.png\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Settings\n",
    "input_folder = '/scratch/leuven/359/vsc35907/big_data_feature_extraction/results/phate_plots2'  # Replace with your folder path\n",
    "output_folder = '/scratch/leuven/359/vsc35907/big_data_feature_extraction/results/phate_plots2/composite'      # Replace with output folder path\n",
    "images_per_row = 4\n",
    "rows_per_image = 6\n",
    "images_per_grid = images_per_row * rows_per_image  # 24 images per output\n",
    "image_size = (600, 500)  # Matches the sample plot's dimensions\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "label = 'label_1' # Change this depending if you want the wild type or the mutants\n",
    "\n",
    "# Get list of PNG images\n",
    "image_files = [f for f in os.listdir(input_folder) if label in f]\n",
    "image_files.sort()  # Sort for consistent order\n",
    "\n",
    "# Calculate total output images needed\n",
    "total_images = len(image_files)\n",
    "total_output_images = (total_images + images_per_grid - 1) // images_per_grid\n",
    "\n",
    "for grid_index in range(total_output_images):\n",
    "    # Create a new blank image with transparency support (RGBA)\n",
    "    grid_width = images_per_row * image_size[0]\n",
    "    grid_height = rows_per_image * image_size[1]\n",
    "    grid_image = Image.new('RGBA', (grid_width, grid_height), (0, 0, 0, 0))  # Transparent background\n",
    "\n",
    "    # Process images for this grid\n",
    "    start_index = grid_index * images_per_grid\n",
    "    for i in range(images_per_grid):\n",
    "        if start_index + i >= total_images:\n",
    "            break  # No more images to process\n",
    "        img_path = os.path.join(input_folder, image_files[start_index + i])\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGBA')  # Preserve transparency\n",
    "            img = img.resize(image_size, Image.Resampling.LANCZOS)  # High-quality resizing\n",
    "            row = i // images_per_row\n",
    "            col = i % images_per_row\n",
    "            x = col * image_size[0]\n",
    "            y = row * image_size[1]\n",
    "            grid_image.paste(img, (x, y), img)  # Use alpha channel for transparency\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "    # Save the grid image as PNG with no compression\n",
    "    output_path = os.path.join(output_folder, f\"grid_{label}_{grid_index + 1}.png\")\n",
    "    grid_image.save(output_path, format='PNG', compress_level=0)  # No compression\n",
    "    print(f\"Saved {output_path}\")\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnipose",
   "language": "python",
   "name": "omnipose"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
